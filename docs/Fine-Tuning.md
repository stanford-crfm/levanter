# Custom Fine-Tuning: Alpaca Tutorial

While Levanter's main focus is pretraining, we can also use it for fine-tuning.
As an example, we'll show how to reproduce [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html),
using [Levanter](https://github.com/stanford-crfm/levanter) and either Llama 1 or [Llama 2](https://ai.meta.com/llama/).
The script we develop will be designed for Alpaca, defaulting to using its dataset and prompts, but it should work for
any single-turn instruction-following task.

This tutorial is meant to cover "full finetuning", where you start with a pretrained model and modify
all of its parameters to fit some final task, rather than something like LoRA (though see our [LoRA tutorial](./LoRA.md) for that).
It also documents how to work with datasets that aren't just single `"text"`s, as we use in pretraining.

## Overview of Alpaca

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) is a lightweight fine tune of Llama 1 on a
[dataset of 52000 input/output pairs](https://huggingface.co/datasets/tatsu-lab/alpaca), which
were generated by taking [a seed set from self-instruct](https://github.com/yizhongw/self-instruct) and asking
`text-davinci-003` to generate more examples.

![Schematic diagram of how the Alpaca model was created](https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg)

### The Foundation Model

Llama 1 is a 7B parameter causal language model trained on 1T tokens from various mostly English sources. It's described
in [the Llama paper](https://arxiv.org/abs/2302.13971).

### The Data

More precisely, the dataset is composed of triples of (instruction, input, output), where the instruction is a prompt
describing the task. A bit less than 40% of the examples have inputs, and the rest are just the instruction and output.

Here are some example inputs, instructions, and outputs:

| Instruction                                                     | Input                                             | Output                                                     |
|-----------------------------------------------------------------|---------------------------------------------------|------------------------------------------------------------|
| Translate the following phrase into French.                     | I love you.                                       | Je t'aime.                                                 |
| Compute the area of a rectangle with length 10cm and width 5cm. |                                                   | The area of the rectangle is 50 cm2.                       |
| Classify the following statement as true or false.              | The Supreme Court is the highest court in the US. | True                                                       |
| Name two types of desert biomes.                                |                                                   | Two types of desert biomes are xeric and subpolar deserts. |

Not all of the examples make a lot of sense, some are just plain wrong, and some are weird. (The dataset was
generated by an LLM after all.) But it's a good example of the kind of data you might want to fine tune on.

### Preprocessing

Because Llama is a causal language model, we need to do some preprocessing to turn the pairs/triples into
a single sequence. The usual thing is to interpolate the strings into a prompt. We'll have two prompts,
depending on whether or not there's an input or just an instruction and output.

For example, the first example above would be turned into:

```
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Translate the following phrase into French.

### Input:
I love you.

### Response:
Je t'aime.
```

While the second would be:

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Compute the area of a rectangle with length 10cm and width 5cm.

### Response:
The area of the rectangle is 50 cm2.
```

From there [original Alpaca script](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py) *masks out the loss* for all tokens before the start of the output. This gets
the model to learn to mimic outputs conditioned on inputs, rather than spending time learning to generate inputs and outputs.


##Code Walkthrough

In this section, we'll walk through the code that we use to fine-tune Llama 1 on Alpaca. You can find the full script
[here](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca/alpaca.py).

If you want to just run the script, you can skip to the [Setup](#setup) section.

### Approach

Levanter's existing main entry points are designed for "pure" causal language modeling, where you have a single sequence
and don't want to mask out any tokens. So we'll instead write a custom script that does the following:

* Preprocesses the dataset into a single sequence, interpolating prompts as we go. We'll also construct a `loss_mask` and do any padding.
* Loads the model and resizes the vocabulary to match the tokenizer.
* Runs the training loop.
* Export the final model to Hugging Face.

We'll use the [original Alpaca script](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py) as a reference.
We only cover preprocessing in this tutorial. You can look at [the script](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca/alpaca.py)
if you want more information.

### Preparing the Dataset

The first step is to get the dataset. We'll use the [Hugging Face Dataset version](https://huggingface.co/datasets/tatsu-lab/alpaca)
to do this. (You can also download it directly from the [dataset page](https://huggingface.co/datasets/tatsu-lab/alpaca), but
Levanter's integration with Hugging Face datasets makes it a bit easier to use.)

```python
def _get_data_source(path_or_id):
    """The original alpaca.py used a json file, but it's since been moved to the HF dataset hub. You can use any
    dataset that's compatible with the structure of the alpaca dataset."""
    if fsspec_utils.exists(path_or_id):
        return JsonDataset([path_or_id])
    else:
        return levanter.data.dataset_from_hf(path_or_id, split="train")
```

Preprocessing in Levanter typically comes in two phases:
* creating the on-disk cache,
* transforming examples from the cache into the examples that the model expects.

Here's the first phase, where we create the cache. We basically want to interpolate the prompt with the input
and instructions, and then tokenize the result. We also want to keep track of the length of the input, so we
can mask out the loss appropriately.

```python
def mk_dataset(data_path_or_id: str, cache_dir: str, tokenizer):
    # wrap an HF dataset with Levanter's native dataset class for fancier preprocessing.
    # Levanter's native dataset class supports streaming, deteriministic, distributed preprocessing out of the box,
    # which is a bit overkill for this dataset, but it's a good example of how to use it.
    dataset = _get_data_source(data_path_or_id)

    prompt_input, prompt_no_input = PROMPT_DICT["prompt_input"], PROMPT_DICT["prompt_no_input"]

    def preprocess(batch):
        sources = [
            prompt_input.format_map(example) if example.get("input", "") != "" else prompt_no_input.format_map(example)
            for example in batch
        ]
        targets = [f"{example['output']}{tokenizer.eos_token}" for example in batch]
        # TODO: this seems pretty wasteful since you end up tokenizing twice, but it's how the original code does it.
        examples = [s + t for s, t in zip(sources, targets)]
        sources_tokenized = tokenizer(sources, return_tensors="np", padding=False, truncation=True)
        examples_tokenized = tokenizer(examples, return_tensors="np", padding=False, truncation=True)

        source_lens = [len(s) for s in sources_tokenized["input_ids"]]

        return {
            "input_ids": examples_tokenized["input_ids"],
            "source_lens": source_lens,
        }

    dataset = dataset.map_batches(preprocess, batch_size=128, num_cpus=num_cpus_used_by_tokenizer(tokenizer))
    dataset = dataset.build_cache(cache_dir, await_finished=True)

    # SupervisedDataset does last minute padding and masking
    dataset = SupervisedDataset(dataset, tokenizer)

    return dataset
```

In the second, we create [levanter.models.lm.LmExample][] objects from the cache. These are the inputs to the model.
`LmExample`s look like this:

```python
class LmExample(eqx.Module):
    tokens: hax.NamedArray
    loss_mask: hax.NamedArray
    attn_mask: AttentionMask = AttentionMask.causal()
```

So we need to populate the first two fields. We'll do that with a dataset whose job is to take the cache and turn it into
`LmExample`s.

```python
class SupervisedDataset(Dataset[LmExample]):
    def __init__(self, preproc_dataset, tokenizer):
        self.preproc_dataset = preproc_dataset
        self.tokenizer = tokenizer

    def __iter__(self):
        for ex in self.preproc_dataset:
            # annoyingly, pad expects things to be batched so we have to prepend a batch axis
            ex = self.tokenizer.pad(
                {k: np.expand_dims(v, 0) for k, v in ex.items()}, return_tensors="np", padding="max_length"
            )
            ex = {k: v[0] for k, v in ex.items()}
            input_ids = hax.named(ex["input_ids"], "position")

            # mask out padding and anything before the start of the target
            Pos = input_ids.resolve_axis("position")
            loss_mask = hax.arange(Pos) >= ex["source_lens"]

            # don't predict the padding
            targets = hax.roll(input_ids, -1, Pos)
            loss_mask = loss_mask & (targets != self.tokenizer.pad_token_id)

            yield LmExample(input_ids, loss_mask)
```

### The Rest

The rest is boilerplate: setting up the model, optimizer, and trainer, and then running the training loop.
We'll skip over that in this tutorial, but you can see the full script [here](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca/alpaca.py)

## Setup

### Cloning and Installing Levanter

If you haven't done so already as part of the GPU or TPU environment setup, clone and install Levanter:

```bash
git clone https://github.com/stanford-crfm/levanter.git
cd levanter
pip install -e .
```

### \[GPU\] Environment Setup
Follow the instructions in the [Getting Started with GPUs](./Getting-Started-GPU.md) guide to create a conda environment and install JAX with CUDA.

### \[TPU\] Environment Setup
We'll spin up a TPU VM using the [Getting Started with TPUs](./Getting-Started-TPU-VM.md) guide.
If you haven't gone through that guide before, you should do so now. If you have, you can just run the
following command from a local checkout of the Levanter repo:

```bash
bash infra/spin-up-vm.sh llama-32 -z us-east1-d -t v3-32 --preemptible
```

## Configuration

We have two configs for Alpaca: one for Llama 1 and one for Llama 2. The only difference is the model id.

### Config to Replicate Alpaca

```yaml
# cf https://github.com/tatsu-lab/stanford_alpaca#fine-tuning
model_name_or_path: huggyllama/llama-7b
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-alpaca"
  num_train_steps: 1218  # 128 * 1218 = 155904, which is almost but not quite 3 epochs, which is what alpaca did
  train_batch_size: 128
  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
```

This config uses mixed fp32/bf16 precision and sets the number of training steps to be roughly 3 epochs. It sets up the optimizer
to use a learning rate of 2e-5 and no weight decay. `trainer.per_device_parallelism` is roughly equivalent to HF's
`per_device_train_batch_size`. If you want to use model parallelism, you can set `trainer.model_axis_size` to something
like 2. (This will split the model across two devices. This might be useful if you're using a v3-64 or something similar and
want to maintain the same batch size.)

### Llama 2 Config

The [Llama 2 config](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca/alpaca-llama2.yaml) is identical, except for the model id.
If you haven't already, go to [Llama 2's Hugging Face page](https://huggingface.co/meta-llama/Llama-2-7b-hf) and request access to the model.

Once you have access, go to [Hugging Face's Tokens page](https://huggingface.co/settings/tokens) to get an API token. You'll need to provide this
to the TPU VM as an environment variable. (We'll show you how to do this later.)

### Customizing the Config

If you have your own dataset, you'll want to change the `data` field in the config to point to your dataset.
You'll also want to change the `model_name_or_path` field to point to the model you want to use.
Currently, Levanter supports GPT-2, Llama, MPT, and Backpack checkpoints.


#### \[TPU\] Using a Modified Config

If you make changes to the config, you'll need to get the config file to all the workers. For TPU, the best way to do this
is to copy it to Google Cloud Storage so that it persists when the machine is preempted. You can do this with:

```bash
gsutil cp levanter/examples/alpaca/alpaca.yaml gs://<somewhere>/train-alpaca.yaml
```

If using Llama 2:

```bash
gsutil cp levanter/examples/alpaca/alpaca-llama2.yaml gs://<somewhere>/train-alpaca.yaml
```

And then using `--config_path gs://<somewhere>/alpaca.yaml` instead of `--config_path levanter/examples/alpaca/train-alpaca.yaml`
in the command line below. Levanter knows how to read from Google Cloud Storage, so you don't need to do anything else.

## Launching the Job

### \[GPU\] Launching the Job

Right now, Levanter is only known to work with single node GPU training. The example commands below demonstrate how to launch a training job
on a node with 8 A100 GPUs, but should work for other single node GPU configurations. For example, we've also tested Alpaca replication with
a node of RTX 6000 Ada Generation 49.1GB GPUs.

Before running your training bash command, ensure you are in your `levanter` conda environment, you've created a directory for saving checkpoints
during training, you are logged into your wandb account with the following two commands:

```bash
conda activate levanter
mkdir -p levanter/checkpoints
wandb login ${YOUR TOKEN HERE}
```
Now you can run the training command:

```bash
python levanter/examples/alpaca.py \
--config_path levanter/examples/alpaca.yaml \
--trainer.checkpointer.base_path levanter/checkpoints \
--hf_save_path levanter/checkpoints
```

You can change `--trainer.checkpointer.base_path` and `--hf_save_path` to your desired model checkpoint directories.

### \[GPU\] NLP-Group Slurm Cluster Launch Example

Say you save the above Alpaca training command as a bash script called `train_alpaca.sh`, then
you could launch a training job on a slurm cluster with `srun` as follows:

```bash
srun --account=nlp --cpus-per-task=32 --gpus-per-node=8 --mem=400G --open-mode=append --partition=sphinx  --nodes=1 --pty bash train_alpaca.sh
```

### \[TPU\] Launching the Job

For TPU, we need just a little bit of ceremony to get the Hugging Face and WANDB API tokens in the environment:
(If you're using Llama 1, you don't need the `HUGGING_FACE_HUB_TOKEN` line.)

```bash
gcloud compute tpus tpu-vm ssh llama-32 --zone us-east1-d --worker=all \
--command="WANDB_API_KEY=${YOUR TOKEN HERE} \
HUGGING_FACE_HUB_TOKEN=${YOUR TOKEN HERE} \
bash levanter/infra/run.sh python \
levanter/examples/alpaca/alpaca.py \
--config_path levanter/examples/alpaca/alpaca.yaml \
--trainer.checkpointer.base_path gs://<somewhere> \
--hf_save_path gs://<somewhere> \
```

If you're using preemptible or TRC TPUs, you'll want to add `--trainer.id <some id>` to the command line,
and probably use the [babysitting script](./Getting-Started-TPU-VM.md#babysitting-script) to automatically restart the
vm and job if it gets preempted. That would look like this:

```bash
infra/babysit-tpu-vm.sh llama-32 -z us-east1-d -t v3-32 --preemptible -- \
WANDB_API_KEY=${YOUR TOKEN HERE} \
HUGGING_FACE_HUB_TOKEN=${YOUR TOKEN HERE} \
bash levanter/infra/run.sh python \
levanter/examples/alpaca/alpaca.py \
--config_path levanter/examples/alpaca/alpaca-llama2.yaml \
--trainer.checkpointer.base_path gs://<somewhere> \
--hf_save_path gs://<somewhere> \
```


## Waiting

At some point it will spit out a WandB link. You can click on that to see the training progress. There's
not a ton to see there (yet), but you can see the training loss go down over time.

On a v3-32 or an 8xA100 box, training should take about ~3.5 hours. -32 TPUs. It should take ~8.5 hours on 8 RTX 6000 Ada Generation GPUs.

## Using the Model

When you're done, you can copy out the Hugging Face model with:

```bash
gsutil cp -r gs://<somewhere>/<run_id>/step-<something> ./my-alpaca
```

The model should work out-of-the-box as a Hugging Face model. You can use it like this:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("./my-alpaca")
tokenizer = AutoTokenizer.from_pretrained("./my-alpaca")

instruction = "Translate the following phrase into French."
input = "I love you."

input = ("Below is an instruction that describes a task, paired with an input that provides further context. "
        "Write a response that appropriately completes the request.\n\n"
        f"### Instruction:\n {instruction}\n### Input:\n {input}\n### Response: \n")

input_ids = tokenizer(input, return_tensors="pt")["input_ids"]
output_ids = model.generate(input_ids, do_sample=True, max_length=100, num_beams=5, num_return_sequences=5)

for output_id in output_ids:
    print(tokenizer.decode(output_id, skip_special_tokens=True))
```

You can also hook it up to your favorite inference server for faster inference.
