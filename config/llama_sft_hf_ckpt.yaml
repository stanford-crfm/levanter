# Model configuration
model:
  type: llama
  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: true
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: false