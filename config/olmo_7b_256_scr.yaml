data: 
  !include data/dolma_olmo_paloma.yaml
model:
  type: olmo
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_kv_heads: 32
  num_heads: 32
  seq_len: 2048
  activation_function: "silu"
  initializer_range: 0.02
use_hf_model_config: true

trainer:
  ray:
    auto_start_cluster: false
  checkpointer:
    keep:
      - every: 25
        until: 100
      - every: 50
        until: 1000
      - every: 1000
        until: 10000
        
  tracker:
    type: wandb
    project: "trace-train"
    tags: ["pile", "olmo", "web_comparison"]
  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_eval_parallelism: -1
  max_eval_batches: 2
  train_batch_size: 1024
  num_train_steps: 50000
  steps_per_eval: 10
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  seed: 0

optimizer:
# lr from olmo (they decay up to 21 billion tokens from 3E-4)
  learning_rate: 3E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1

hf_save_steps: 10000
hf_save_path: "gs://levanter-checkpoints/olmo_trace_fixedinit_shufdata/"  # TODO
data_seed: 0
