data: 
  stop_strategy: restart
  shuffle_buffer_size: 100000
  !include data/dolma_olmo_paloma_shuf.yaml
model:
  type: olmo
initialize_from_hf: "allenai/OLMo-1.7-7B-hf@step0-tokens0B"
use_hf_model_config: true

trainer:
  ray:
    auto_start_cluster: false
  checkpointer:
    keep:
      - every: 25
        until: 100
      - every: 50
        until: 2000
  tracker:
    type: wandb
    project: "trace-train"
    tags: ["pile", "olmo", "web_comparison"]
  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_eval_parallelism: -1
  max_eval_batches: 2
  train_batch_size: 1024
  num_train_steps: 50000
  steps_per_eval: 10
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"

optimizer:
# lr from olmo (they decay up to 21 billion tokens from 3E-4)
  learning_rate: 3E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1

hf_save_steps: 10000
hf_save_path: "gs://levanter-checkpoints/olmo_trace_hf/"  # TODO
data_seed: 0
