data:
  cache_dir: "gs://levanter-data/tokenized/markweb_llama/"
  tokenizer: "meta-llama/Llama-2-7b-hf"
  # needs to be smaller for pile b/c of books
  rows_per_chunk: 2048
  configs:
    fineweb:
      id: "HuggingFaceFW/fineweb"
      stream: True
    pile/arxiv:
      train_urls:
        - gs://levanter-data/pile-domains/arxiv/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/arxiv/val.jsonl.zst
    pile/books2:
      train_urls:
        - gs://levanter-data/pile-domains/books2/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/books2/val.jsonl.zst
    pile/books3:
      train_urls:
        - gs://levanter-data/pile-domains/books3/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/books3/val.jsonl.zst
    pile/dm_math:
      train_urls:
        - gs://levanter-data/pile-domains/dm_math/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/dm_math/val.jsonl.zst
    pile/enron:
      train_urls:
        - gs://levanter-data/pile-domains/enron/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/enron/val.jsonl.zst
    pile/europarl:
      train_urls:
        - gs://levanter-data/pile-domains/europarl/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/europarl/val.jsonl.zst
    pile/free_law:
      train_urls:
        - gs://levanter-data/pile-domains/freelaw/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/freelaw/val.jsonl.zst
    pile/github:
      train_urls:
        - gs://levanter-data/pile-domains/github/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/github/val.jsonl.zst
    pile/hackernews:
      train_urls:
        - gs://levanter-data/pile-domains/hackernews/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/hackernews/val.jsonl.zst
    pile/nih:
      train_urls:
        - gs://levanter-data/pile-domains/nih/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/nih/val.jsonl.zst
    pile/opensubtitles:
      train_urls:
        - gs://levanter-data/pile-domains/opensubtitles/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/opensubtitles/val.jsonl.zst
    pile/owt2:
      train_urls:
        - gs://levanter-data/pile-domains/owt2/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/owt2/val.jsonl.zst
    pile/pg_19:
      train_urls:
        - gs://levanter-data/pile-domains/pg_19/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/pg_19/val.jsonl.zst
    pile/philpapers:
      train_urls:
        - gs://levanter-data/pile-domains/philpapers/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/philpapers/val.jsonl.zst
    pile/pile_cc:
      train_urls:
        - gs://levanter-data/pile-domains/pile_cc/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/pile_cc/val.jsonl.zst
    pile/pubmed_abs:
      train_urls:
        - gs://levanter-data/pile-domains/pubmed_abs/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/pubmed_abs/val.jsonl.zst
    pile/pubmed_central:
      train_urls:
        - gs://levanter-data/pile-domains/pubmed_central/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/pubmed_central/val.jsonl.zst
    pile/stack_exchange:
      train_urls:
        - gs://levanter-data/pile-domains/stack_exchange/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/stack_exchange/val.jsonl.zst
    pile/ubuntu_irc:
      train_urls:
        - gs://levanter-data/pile-domains/ubuntu_irc/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/ubuntu_irc/val.jsonl.zst
    pile/uspto:
      train_urls:
        - gs://levanter-data/pile-domains/uspto/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/uspto/val.jsonl.zst
    pile/wiki_en:
      train_urls:
        - gs://levanter-data/pile-domains/wiki_en/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/wiki_en/val.jsonl.zst
    pile/youtube_subtitles:
      train_urls:
        - gs://levanter-data/pile-domains/youtube_subtitles/{00..29}.jsonl.zst
      validation_urls:
        - gs://levanter-data/pile-domains/youtube_subtitles/val.jsonl.zst
    # these are just for eval
    "paloma/4chan":
      validation_urls:
        - gs://levanter-data/paloma/4chan_meta_sep/val/val*.jsonl.gz
    "paloma/c4_100_domains":
      validation_urls:
        - gs://levanter-data/paloma/c4_100_domains/val/val*.jsonl.gz
    "paloma/c4_en":
      validation_urls:
        - gs://levanter-data/paloma/c4_en/val/val*.jsonl.gz
    "paloma/dolma-v1_5":
      validation_urls:
        - gs://levanter-data/paloma/dolma-v1_5/val/val*.jsonl.gz
    "paloma/dolma_100_programing_languages":
      validation_urls:
        - gs://levanter-data/paloma/dolma_100_programing_languages/val/val*.jsonl.gz
    "paloma/dolma_100_subreddits":
      validation_urls:
        - gs://levanter-data/paloma/dolma_100_subreddits/val/val*.jsonl.gz
    "paloma/falcon-refinedweb":
      validation_urls:
        - gs://levanter-data/paloma/falcon-refinedweb/val/val*.jsonl.gz
    "paloma/gab":
      validation_urls:
        - gs://levanter-data/paloma/gab/val/val*.jsonl.gz
    "paloma/m2d2_s2orc_unsplit":
      validation_urls:
        - gs://levanter-data/paloma/m2d2_s2orc_unsplit/val/val*.jsonl.gz
    "paloma/m2d2_wikipedia_unsplit":
      validation_urls:
        - gs://levanter-data/paloma/m2d2_wikipedia_unsplit/val/val*.jsonl.gz
    "paloma/manosphere_meta_sep":
      validation_urls:
        - gs://levanter-data/paloma/manosphere_meta_sep/val/val*.jsonl.gz
    "paloma/mc4":
      validation_urls:
        - gs://levanter-data/paloma/mc4/val/val*.jsonl.gz
    "paloma/ptb":
      validation_urls:
        - gs://levanter-data/paloma/ptb/val/val*.jsonl.gz
    "paloma/redpajama":
      validation_urls:
        - gs://levanter-data/paloma/redpajama/val/val*.jsonl.gz
    "paloma/twitterAAE_HELM_fixed":
      validation_urls:
        - gs://levanter-data/paloma/twitterAAE_HELM_fixed/val/val*.jsonl.gz
    "paloma/wikitext_103":
      validation_urls:
        - gs://levanter-data/paloma/wikitext_103/val/val*.jsonl.gz

  train_weights:
    fineweb: 1.0
    pile/arxiv: 0.0
    pile/books2: 0.0
    pile/books3: 0.0
    pile/dm_math: 0.0
    pile/enron: 0.0
    pile/europarl: 0.0
    pile/free_law: 0.0
    pile/github: 0.0
    pile/hackernews: 0.0
    pile/nih: 0.0
    pile/opensubtitles: 0.0
    pile/owt2: 0.0
    pile/pg_19: 0.0
    pile/philpapers: 0.0
    pile/pile_cc: 0.0
    pile/pubmed_abs: 0.0
    pile/pubmed_central: 0.0
    pile/stack_exchange: 0.0
    pile/ubuntu_irc: 0.0
    pile/uspto: 0.0
    pile/wiki_en: 0.0
    pile/youtube_subtitles: 0.0

    paloma/4chan: 0.0
    paloma/c4_100_domains: 0.0
    paloma/c4_en: 0.0
    paloma/dolma-v1_5: 0.0
    paloma/dolma_100_programing_languages: 0.0
    paloma/dolma_100_subreddits: 0.0
    paloma/falcon-refinedweb: 0.0
    paloma/gab: 0.0
    paloma/m2d2_s2orc_unsplit: 0.0
    paloma/m2d2_wikipedia_unsplit: 0.0
    paloma/manosphere_meta_sep: 0.0
    paloma/mc4: 0.0
    paloma/ptb: 0.0
    paloma/redpajama: 0.0
    paloma/twitterAAE_HELM_fixed: 0.0
    paloma/wikitext_103: 0.0
model:
  # 1B class model
  type: llama
  seq_len: 2048
  hidden_dim: 2048
  intermediate_dim: 4096
  num_layers: 24
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: True
  flash_attention_block_size: 2048
trainer:
  seed: 42
  checkpointer:
    keep:
      - every: 1
        until: 10
      - every: 10
        until: 20
      - every: 100
        until: 1000
      - every: 1000
        until: 5000
  tracker:
    type: wandb
    project: "trace-train"
    tags: ["pile", "llama", "web_comparison"]

  mp: p=f32,c=bfloat16
  train_batch_size: 512
  num_train_steps: 5000
  steps_per_eval: 1000
  per_device_eval_parallelism: 64
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
optimizer:
  learning_rate: 2E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1
